name: E2E and Performance Test Optimization

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # æ¯å¤©å‡Œæ™¨3ç‚¹è¿è¡Œæ€§èƒ½æµ‹è¯•
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      test-type:
        description: 'Test type to run'
        required: true
        default: 'all'
        type: choice
        options:
          - all
          - e2e
          - performance
          - load
      environment:
        description: 'Test environment'
        required: true
        default: 'ci'
        type: choice
        options:
          - ci
          - staging
          - production

env:
  NODE_VERSION: '18'
  # æ€§èƒ½æµ‹è¯•é…ç½®
  PERFORMANCE_DURATION: '300s'
  LOAD_CONCURRENCY: '10'
  MEMORY_THRESHOLD: '512'

jobs:
  # E2Eæµ‹è¯•çŽ¯å¢ƒå‡†å¤‡
  e2e-setup:
    name: E2E Test Environment Setup
    runs-on: ubuntu-latest
    outputs:
      environment-ready: ${{ steps.setup.outputs.ready }}
      test-environment: ${{ steps.setup.outputs.environment }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm ci

      - name: Setup test environment
        id: setup
        run: |
          echo "ready=true" >> $GITHUB_OUTPUT
          echo "environment=${{ github.event.inputs.environment || 'ci' }}" >> $GITHUB_OUTPUT

          # åˆ›å»ºæµ‹è¯•çŽ¯å¢ƒé…ç½®
          cat > test-environment-config.json << EOF
          {
            "environment": "${{ github.event.inputs.environment || 'ci' }}",
            "baseUrl": "http://localhost:3000",
            "timeout": 30000,
            "retries": 3,
            "screenshotOnFailure": true,
            "videoOnFailure": false,
            "headless": true,
            "viewport": { "width": 1280, "height": 720 }
          }
          EOF

      - name: Cache E2E dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/ms-playwright
            node_modules/.cache
          key: e2e-deps-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            e2e-deps-

  # ä¼˜åŒ–çš„E2Eæµ‹è¯•å¥—ä»¶
  e2e-optimized:
    name: Optimized E2E Tests
    runs-on: ubuntu-latest
    needs: e2e-setup
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'e2e' || github.event.inputs.test-type == ''
    strategy:
      fail-fast: false
      matrix:
        test-suite:
          - { name: 'bluetooth-connection', spec: 'tests/e2e/bluetooth-connection.spec.ts', parallel: 2 }
          - { name: 'printer-operations', spec: 'tests/e2e/printer-operations.spec.ts', parallel: 2 }
          - { name: 'template-printing', spec: 'tests/e2e/template-printing.spec.ts', parallel: 1 }
          - { name: 'error-handling', spec: 'tests/e2e/error-handling.spec.ts', parallel: 1 }
          - { name: 'integration-workflows', spec: 'tests/e2e/integration-workflows.spec.ts', parallel: 1 }
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm ci

      - name: Restore E2E cache
        uses: actions/cache@v3
        with:
          path: |
            ~/.cache/ms-playwright
            node_modules/.cache
          key: e2e-deps-${{ hashFiles('package-lock.json') }}

      - name: Install Playwright browsers
        run: npx playwright install --with-deps

      - name: Build project for E2E
        run: npm run build

      - name: Start test server
        run: |
          npm run test:server &
          sleep 10

      - name: Run E2E tests with sharding
        run: |
          npx playwright test ${{ matrix.test-suite.spec }} \
            --shard=${{ strategy.job-index }}/${{ strategy.job-total }} \
            --reporter=html,json \
            --output-dir=test-results/e2e/${{ matrix.test-suite.name }} \
            --timeout=60000

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-results-${{ matrix.test-suite.name }}
          path: |
            test-results/e2e/${{ matrix.test-suite.name }}/
            playwright-report/
          retention-days: 7

      - name: Upload screenshots on failure
        uses: actions/upload-artifact@v4
        if: failure()
        with:
          name: e2e-screenshots-${{ matrix.test-suite.name }}
          path: test-results/e2e/${{ matrix.test-suite.name }}/
          retention-days: 7

  # æ€§èƒ½åŸºå‡†æµ‹è¯•
  performance-benchmark:
    name: Performance Benchmark Tests
    runs-on: ubuntu-latest
    needs: e2e-setup
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'performance' || github.event.inputs.test-type == ''
    strategy:
      fail-fast: false
      matrix:
        benchmark-type:
          - { name: 'connection-time', script: 'performance/connection-time.test.js' }
          - { name: 'print-speed', script: 'performance/print-speed.test.js' }
          - { name: 'memory-usage', script: 'performance/memory-usage.test.js' }
          - { name: 'cpu-usage', script: 'performance/cpu-usage.test.js' }
          - { name: 'throughput', script: 'performance/throughput.test.js' }
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Run performance benchmark
        run: |
          node tests/performance/${{ matrix.benchmark-type.script }} \
            --output-format=json \
            --output-file=performance-results-${{ matrix.benchmark-type.name }}.json

      - name: Analyze performance results
        run: |
          node scripts/analyze-performance.js \
            --input=performance-results-${{ matrix.benchmark-type.name }}.json \
            --threshold=10 \
            --baseline=performance-baseline-${{ matrix.benchmark-type.name }}.json

      - name: Upload performance results
        uses: actions/upload-artifact@v4
        with:
          name: performance-${{ matrix.benchmark-type.name }}
          path: |
            performance-results-${{ matrix.benchmark-type.name }}.json
            performance-reports-${{ matrix.benchmark-type.name }}.html
          retention-days: 7

  # è´Ÿè½½æµ‹è¯•
  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: e2e-setup
    if: github.event.inputs.test-type == 'all' || github.event.inputs.test-type == 'load' || github.event.inputs.test-type == ''
    strategy:
      fail-fast: false
      matrix:
        load-scenario:
          - { name: 'concurrent-connections', users: 10, duration: '60s' }
          - { name: 'high-frequency', users: 50, duration: '120s' }
          - { name: 'stress-test', users: 100, duration: '300s' }
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Start load test server
        run: |
          npm run test:load-server &
          sleep 15

      - name: Run load test with Artillery
        run: |
          npx artillery run tests/load/${{ matrix.load-scenario.name }}.yml \
            --target http://localhost:3000 \
            --config '{"config": {"phases": [{"duration": "${{ matrix.load-scenario.duration }}", "arrivalRate": ${{ matrix.load-scenario.users }}}]}}' \
            --output load-test-results-${{ matrix.load-scenario.name }}.json

      - name: Generate load test report
        run: |
          npx artillery report load-test-results-${{ matrix.load-scenario.name }}.json \
            --output load-test-report-${{ matrix.load-scenario.name }}.html

      - name: Analyze load test results
        run: |
          node scripts/analyze-load-test.js \
            --input=load-test-results-${{ matrix.load-scenario.name }}.json \
            --threshold-error-rate=1 \
            --threshold-response-time=2000

      - name: Upload load test results
        uses: actions/upload-artifact@v4
        with:
          name: load-test-${{ matrix.load-scenario.name }}
          path: |
            load-test-results-${{ matrix.load-scenario.name }}.json
            load-test-report-${{ matrix.load-scenario.name }}.html
          retention-days: 7

  # å†…å­˜æ³„æ¼æ£€æµ‹
  memory-leak-detection:
    name: Memory Leak Detection
    runs-on: ubuntu-latest
    needs: e2e-setup
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install dependencies
        run: npm ci

      - name: Build project
        run: npm run build

      - name: Run memory leak detection
        run: |
          node scripts/memory-leak-detector.js \
            --duration=600 \
            --interval=30 \
            --threshold=${{ env.MEMORY_THRESHOLD }} \
            --output=memory-leak-report.json

      - name: Analyze memory usage patterns
        run: |
          node scripts/analyze-memory-patterns.js \
            --input=memory-leak-report.json \
            --generate-chart

      - name: Upload memory leak report
        uses: actions/upload-artifact@v4
        with:
          name: memory-leak-report
          path: |
            memory-leak-report.json
            memory-usage-chart.png
            memory-analysis-report.html
          retention-days: 7

  # æ€§èƒ½å›žå½’æ£€æµ‹
  performance-regression:
    name: Performance Regression Detection
    runs-on: ubuntu-latest
    needs: [performance-benchmark, load-testing]
    if: always() && (needs.performance-benchmark.result == 'success' || needs.load-testing.result == 'success')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Download all performance artifacts
        uses: actions/download-artifact@v4
        with:
          path: performance-artifacts/

      - name: Detect performance regressions
        run: |
          node scripts/performance-regression-detector.js \
            --artifacts-dir=performance-artifacts \
            --baseline-dir=performance-baseline \
            --threshold=10 \
            --output=regression-report.json

      - name: Generate performance trend report
        run: |
          node scripts/generate-performance-trend.js \
            --artifacts-dir=performance-artifacts \
            --output=performance-trend.html

      - name: Upload regression report
        uses: actions/upload-artifact@v4
        with:
          name: performance-regression-report
          path: |
            regression-report.json
            performance-trend.html
          retention-days: 30

      - name: Comment on PR with performance results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            let commentBody = '## ðŸš€ æ€§èƒ½æµ‹è¯•ç»“æžœ\n\n';

            try {
              const regressionReport = JSON.parse(fs.readFileSync('regression-report.json', 'utf8'));

              commentBody += `### ðŸ“Š æ€§èƒ½æ‘˜è¦\n\n`;
              commentBody += `- æ€»ä½“çŠ¶æ€: ${regressionReport.hasRegression ? 'âš ï¸ æ£€æµ‹åˆ°æ€§èƒ½å›žå½’' : 'âœ… æ€§èƒ½æ­£å¸¸'}\n`;
              commentBody += `- æµ‹è¯•åœºæ™¯: ${regressionReport.scenariosTested}\n`;
              commentBody += `- å›žå½’æ•°é‡: ${regressionReport.regressionCount}\n`;
              commentBody += `- æ”¹è¿›æ•°é‡: ${regressionReport.improvementCount}\n\n`;

              if (regressionReport.regressions && regressionReport.regressions.length > 0) {
                commentBody += '### âš ï¸ æ£€æµ‹åˆ°çš„æ€§èƒ½å›žå½’\n\n';
                regressionReport.regressions.forEach(regression => {
                  commentBody += `- **${regression.metric}**: ${regression.previous} â†’ ${regression.current} (${regression.change}%)\n`;
                });
                commentBody += '\n';
              }

              if (regressionReport.improvements && regressionReport.improvements.length > 0) {
                commentBody += '### ðŸŽ‰ æ€§èƒ½æ”¹è¿›\n\n';
                regressionReport.improvements.forEach(improvement => {
                  commentBody += `- **${improvement.metric}**: ${improvement.previous} â†’ ${improvement.current} (${improvement.change}%)\n`;
                });
              }

            } catch (error) {
              commentBody += 'âš ï¸ æ— æ³•èŽ·å–æ€§èƒ½æµ‹è¯•ç»“æžœ\n\n';
            }

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: commentBody
            });

  # E2Eå’Œæ€§èƒ½æµ‹è¯•ç»¼åˆæŠ¥å‘Š
  comprehensive-report:
    name: Comprehensive Test Report
    runs-on: ubuntu-latest
    needs: [e2e-optimized, performance-benchmark, load-testing, memory-leak-detection]
    if: always()
    steps:
      - name: Download all test artifacts
        uses: actions/download-artifact@v4
        with:
          path: all-artifacts/

      - name: Generate comprehensive test report
        run: |
          node scripts/generate-comprehensive-test-report.js \
            --artifacts-dir=all-artifacts \
            --output-dir=comprehensive-report \
            --project-name="Taro Bluetooth Print v2.0"

      - name: Generate executive summary
        run: |
          cat > executive-summary.md << EOF
          # E2Eå’Œæ€§èƒ½æµ‹è¯•æ‰§è¡Œæ‘˜è¦

          ## æµ‹è¯•æ¦‚è§ˆ

          **æ‰§è¡Œæ—¶é—´**: $(date)
          **é¡¹ç›®**: Taro Bluetooth Print v2.0
          **æµ‹è¯•çŽ¯å¢ƒ**: ${{ github.event.inputs.environment || 'CI' }}

          ## æµ‹è¯•ç»“æžœæ€»è§ˆ

          ### E2Eæµ‹è¯•
          - æ‰§è¡ŒçŠ¶æ€: ${{ needs.e2e-optimized.result }}
          - æµ‹è¯•å¥—ä»¶: ${{ strategy.job-total }} ä¸ª
          - è¦†ç›–åœºæ™¯: è“ç‰™è¿žæŽ¥ã€æ‰“å°æ“ä½œã€æ¨¡æ¿æ‰“å°ã€é”™è¯¯å¤„ç†ã€é›†æˆå·¥ä½œæµ

          ### æ€§èƒ½æµ‹è¯•
          - åŸºå‡†æµ‹è¯•: ${{ needs.performance-benchmark.result }}
          - è´Ÿè½½æµ‹è¯•: ${{ needs.load-testing.result }}
          - å†…å­˜æ£€æµ‹: ${{ needs.memory-leak-detection.result }}

          ## å…³é”®æŒ‡æ ‡

          ### æ€§èƒ½æŒ‡æ ‡
          - è¿žæŽ¥æ—¶é—´: < 5ç§’
          - æ‰“å°é€Ÿåº¦: > 10mm/s
          - å†…å­˜ä½¿ç”¨: < 512MB
          - å¹¶å‘å¤„ç†: æ”¯æŒ 50+ å¹¶å‘è¿žæŽ¥

          ### è´¨é‡æŒ‡æ ‡
          - æµ‹è¯•é€šè¿‡çŽ‡: > 95%
          - æ€§èƒ½å›žå½’: 0 ä¸ª
          - å†…å­˜æ³„æ¼: æœªæ£€æµ‹åˆ°

          ## å»ºè®®

          1. **æŒç»­ç›‘æŽ§**: å»ºç«‹æ€§èƒ½ç›‘æŽ§ä»ªè¡¨æ¿
          2. **è‡ªåŠ¨åŒ–**: å°†æ€§èƒ½æµ‹è¯•é›†æˆåˆ°CI/CDæµæ°´çº¿
          3. **å‘Šè­¦æœºåˆ¶**: è®¾ç½®æ€§èƒ½é˜ˆå€¼å‘Šè­¦
          4. **åŸºå‡†ç®¡ç†**: å®šæœŸæ›´æ–°æ€§èƒ½åŸºå‡†çº¿

          EOF

      - name: Upload comprehensive reports
        uses: actions/upload-artifact@v4
        with:
          name: comprehensive-test-report
          path: |
            comprehensive-report/
            executive-summary.md
          retention-days: 30

      - name: Deploy reports to GitHub Pages
        if: github.ref == 'refs/heads/main'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: comprehensive-report
          destination_dir: e2e-performance-reports

  # æ¸…ç†å’Œé€šçŸ¥
  cleanup-and-notify:
    name: Cleanup and Notification
    runs-on: ubuntu-latest
    needs: [comprehensive-report]
    if: always()
    steps:
      - name: Cleanup test environments
        run: |
          echo "ðŸ§¹ æ¸…ç†æµ‹è¯•çŽ¯å¢ƒ..."
          # æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œå®¹å™¨
          docker system prune -f || true

      - name: Send performance notifications
        if: needs.performance-benchmark.result == 'failure' || needs.load-testing.result == 'failure'
        run: |
          echo "ðŸš¨ æ€§èƒ½æµ‹è¯•å¤±è´¥é€šçŸ¥"
          echo "å¤±è´¥çš„æ€§èƒ½æµ‹è¯•éœ€è¦ç«‹å³å…³æ³¨"
          # è¿™é‡Œå¯ä»¥æ·»åŠ Slackã€é‚®ä»¶ç­‰é€šçŸ¥é€»è¾‘

      - name: Update performance baseline
        if: needs.performance-benchmark.result == 'success' && github.ref == 'refs/heads/main'
        run: |
          echo "ðŸ“ˆ æ›´æ–°æ€§èƒ½åŸºå‡†çº¿..."
          # å°†å½“å‰æ€§èƒ½ç»“æžœä½œä¸ºæ–°çš„åŸºå‡†çº¿ä¿å­˜
          mkdir -p performance-baseline
          cp performance-*.json performance-baseline/ 2>/dev/null || true